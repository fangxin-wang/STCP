{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: torch in /root/miniconda3/lib/python3.10/site-packages (2.1.2+cu118)\n",
      "Requirement already satisfied: scikit-learn in /root/miniconda3/lib/python3.10/site-packages (1.5.0)\n",
      "Requirement already satisfied: fsspec in /root/miniconda3/lib/python3.10/site-packages (from torch) (2024.6.0)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /root/miniconda3/lib/python3.10/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: typing-extensions in /root/miniconda3/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/lib/python3.10/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /root/miniconda3/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /root/miniconda3/lib/python3.10/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /root/miniconda3/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /root/miniconda3/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /root/miniconda3/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.1551\n",
      "Epoch [20/100], Loss: 0.1091\n",
      "Epoch [30/100], Loss: 0.0906\n",
      "Epoch [40/100], Loss: 0.0875\n",
      "Epoch [50/100], Loss: 0.0812\n",
      "Epoch [60/100], Loss: 0.0758\n",
      "Epoch [70/100], Loss: 0.0713\n",
      "Epoch [80/100], Loss: 0.0669\n",
      "Epoch [90/100], Loss: 0.0626\n",
      "Epoch [100/100], Loss: 0.0582\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "n_samples = 1000\n",
    "X = np.random.rand(n_samples, 1) * 10\n",
    "y = np.sin(X).ravel() + np.random.randn(n_samples) * 0.1\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "n_samples = 1000\n",
    "X = np.random.rand(n_samples, 1) * 10\n",
    "y = np.sin(X).ravel() + np.random.randn(n_samples) * 0.1\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "def quantile_loss(pred, target, quantiles):\n",
    "    assert not target.requires_grad\n",
    "    assert pred.size(0) == target.size(0)\n",
    "    losses = []\n",
    "    for i, q in enumerate(quantiles):\n",
    "        errors = target - pred[:, i]\n",
    "        losses.append(torch.max((q - 1) * errors, q * errors).unsqueeze(1))\n",
    "    return torch.mean(torch.sum(torch.cat(losses, dim=1), dim=1))\n",
    "class QuantileRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QuantileRegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 2)  # Output two quantiles: q_low, q_high\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "model = QuantileRegressionModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "quantiles = [0.025, 0.975]\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = quantile_loss(output, y_train, quantiles)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.69140375 -0.25012094 -0.41708943 -0.09124333 -0.07694805 -0.06675088\n",
      " -0.24454927 -0.6003647  -0.3057853  -0.7970952  -1.0392644  -0.5413891\n",
      " -0.22948724 -0.18526882 -0.447169   -0.13634866 -0.6917387  -0.13946998\n",
      " -0.17674935 -0.4445994  -0.5825853  -0.23574722 -0.57265997 -1.2963347\n",
      " -1.6399329  -0.14971668 -0.49162912 -0.625566   -0.10749876 -0.5033419\n",
      " -0.34127557 -0.23014218 -0.39188123 -0.22816372 -0.49552715 -0.4630437\n",
      " -0.9793608  -0.6303951  -0.3391699  -0.28044105 -0.43031162 -0.8957916\n",
      " -0.3836431  -1.5201294  -1.5882665  -0.17266965 -0.40139192 -0.60008913\n",
      "  0.03701425 -0.34792763 -1.3249104  -0.15087557 -0.27941048 -0.09182686\n",
      " -0.26042867 -0.4675847  -0.6814906  -0.27468395 -0.6362156  -0.07213223\n",
      " -0.16245896 -0.09952539 -1.1040926  -0.9651578  -1.2106256  -0.25163978\n",
      " -0.20393777  0.02909541 -1.6940236  -0.8861201  -0.6505639  -0.06454617\n",
      " -0.35557985 -1.713304   -0.3953836  -0.98340905 -0.572245   -0.6039212\n",
      " -1.0666592  -0.3247531  -0.18688112 -0.35708988 -1.4699705  -0.1959495\n",
      " -0.6483584  -0.13627672 -0.18469244  0.03953433 -0.2746256  -0.6174858\n",
      " -1.4061781  -1.7255703  -0.27038625 -0.8201928  -0.26842767 -0.7207526\n",
      " -0.938995   -0.6744009  -0.08187681 -0.10316324  0.07483935 -0.34297758\n",
      " -0.7014967  -0.49708647 -1.5160222  -0.21306586 -0.5881831  -0.84203947\n",
      " -0.38300425 -0.6803537  -0.07285941 -0.3611905  -0.26834953  0.02979827\n",
      " -0.26626885 -0.52661586 -1.428204   -0.78484476 -0.2467078  -0.24441618\n",
      " -0.17516768 -0.23034406 -0.05295211 -0.3913036  -0.0563693  -0.8614485\n",
      " -0.27030635 -0.45958942 -0.38911986 -0.34251037 -1.6978606  -0.19363368\n",
      " -0.24617815 -0.18663514 -0.05595285 -0.00465155 -0.27907306 -0.28265142\n",
      " -1.0033414  -1.7496557  -0.8180251   0.13440931 -1.1658307  -0.5818477\n",
      " -0.7260743  -0.6214748  -0.35957378 -0.51430416 -0.15158993 -0.1326313\n",
      " -0.11375582 -0.11238241 -0.65126103 -0.58732355 -0.25381535 -0.182441\n",
      " -0.19930117 -0.12650344  0.0261116  -0.17696977 -1.7827904  -0.2728541\n",
      " -0.36249787 -0.31676883 -1.6736925  -0.47900176 -1.7464197  -0.23185068\n",
      " -0.76156294 -0.1949346  -1.0221055  -0.08963513 -0.01265275 -0.23697174\n",
      " -0.3167392   0.16117764 -0.90455794 -0.7434501  -0.18920118 -0.468723\n",
      " -0.55092937 -0.21833587 -1.7253531  -0.3706947  -0.5742582  -0.86154485\n",
      " -0.53725815 -0.04480678 -0.44591737 -0.22589403 -0.4449544   0.09650838\n",
      " -0.16787362 -0.03611618 -0.50270206 -1.0456893  -0.43669254 -0.6037815\n",
      " -0.04170513 -0.7252082 ]\n",
      "Prediction intervals with 0.94 coverage:\n",
      "coverage is 0.940000\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    preds = model(X_test).numpy()\n",
    "    lower_preds = preds[:, 0]\n",
    "    upper_preds = preds[:, 1]\n",
    "    \n",
    "    # Calculate nonconformity scores (absolute errors)\n",
    "    calibration_errors = np.maximum(lower_preds - y_test.numpy(), y_test.numpy() - upper_preds)\n",
    "    print(calibration_errors)\n",
    "    # Compute the q_hat quantile of the calibration errors\n",
    "    alpha = 0.06  # Coverage level\n",
    "    q_hat = np.quantile(calibration_errors, 1 - alpha)\n",
    "    print(q_hat)\n",
    "    # Generate conformal intervals\n",
    "    lower_bound = lower_preds - q_hat\n",
    "    upper_bound = upper_preds + q_hat\n",
    "    coverage=0\n",
    "    print(f\"Prediction intervals with {1-alpha} coverage:\")\n",
    "    for i in range(len(lower_bound)):  # Show a few examples\n",
    "        #print(f\"Predicted interval: [{lower_bound[i]:.3f}, {upper_bound[i]:.3f}], True value: {y_test[i]:.3f}\")\n",
    "        if y_test[i]>=lower_bound[i] and y_test[i]<=upper_bound[i]:\n",
    "           coverage=coverage+1\n",
    "    print(\"coverage is {:.6f}\".format(coverage/len(lower_bound)))\n",
    "    print(len(lower_bound))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
